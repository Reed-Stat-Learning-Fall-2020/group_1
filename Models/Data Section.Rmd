---
title: "Final Data Section"
author: "Group 1-Josh Yamamoto, Riley Leonard, Andy Zhao"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)

```

#Data#

After describing some of the trends we observe in the data, we then turn towards the variety of models that we have created as well as their respective performances, measured in adjusted R-squared as well as 5-fold cross-validation MSE.

##Full Model##
```{r}
# Full Model for ERA

full_mod <- lm(data = pitchers,
                 ERA_t1 ~.)

# Full Model for Salary

full_mod <- lm(data = pitchers,
                 salary_t1 ~.)
```

Above, we see the code for the full model on both salary and ERA. We expected that the full model would do decently well as we had picked every variable included in our dataset and thus we expected all of our variables to be at least somewhat significant. However, one of the issues we had forsaw was the presence of a great deal of collinearity, which could make our parameter estimates quite unstable, and the full model doesn't really have any way to counter said collinearity.

###Full Model Results##
```{r}
full_statistics<-c(0.6092759,0.8313169,0.1397712,1.04342)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

full_model_table<-data.frame(labels,full_statistics) 
full_model_table<-full_model_table %>%  rename(
    `Full Model Statistics` = full_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels,
    )

kable(full_model_table)

```

It seems like our full model struggles with the data compared to our other models. This is understandable for the reasons listed above, namely the presence of correlation between predictors. Interestingly, even the full model has a 5-fold CV MSE for ERA above that of the null model of SIERA (see introduction for a full discussion of the null model), suggesting that even the weakest full model still outperformed existing ERA projection methodologies. It is also notable that the adjusted R-squared for ERA is far weaker than that of salary, this is understandable as the irreducible error or noise in the ERA data far exceeds that of the salary data. While there is some stochasticity in contract negotiations, there is far more noise when it comes to on-field performance.

##Forward Selection##
```{r}
# Forward Selection Model for ERA

forward_mod <- lm(data = pitchers, 
                   ERA_t1 ~ wOBA+L+BFP+SO+K_percent+hard_hit_percent+
                    barrel_percent+`ERA/Barrel %`+luck_adj_ERA)

# Forward Selection Model for salary
forward_mod_salary <- lm(data = salary_data,
                         log(salary_t1)~Pitches+Year+xBA+spin_rate+
                           Velocity+K_percent+Salary+luck_adj_ERA)
```

We next moved on to forward selection, which is an efficient alternative to best subset. We set nvmax or the maximum complexity of the model to be considered at 12. We chose this because of the aforementioned problems with collinearity. Subset selection gives us a way to reduce collinearity by simply dropping some of the correlated predictors. We set the maximum to 12 because we wanted a relatively parsimonious model. Ultimately, the adjusted R-squared of the model with 9 predictors performed the best among all the 12 ERA models while the 8 predictor model performed the best for salary.

While a more detailed discussion will be left for the conclusion. In broad strokes, the predictors included in the two models are not as different as one would expect. The presence of expected batting averaged (xBA) and spin rate in the salary model suggests that these sabermetric statistics actually result in a larger contract, but, conversely, base counting statistics like total pitches and even year is also included.

The forward model for ERA includes a number of advanced statistics such as wOBA, hard hit %, barrel %, and even ERA/barrel %. However, it also includes counting statistics like total strikeouts and total batters faced. Ultimately, the forward model for the two are more similar than one would have expected. 

###Full Model Results##
```{r}
forward_statistics<-c(0.6169164,0.7657645,0.1642511,0.9994591)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

forward_model_table<-data.frame(labels,forward_statistics) 
forward_model_table<-forward_model_table %>%  rename(
    `Forward Model Statistics` = forward_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels,
    )

kable(forward_model_table)

```

The forward selection method of subset selection led to improvements over the full model, likely due to reduction in collinearity by dropping some of the correlated predictors. Interestingly, the improvement is quite marginal and did not lead to as major of an increase in 5-fold CV MSE or adjusted R-squared as one would have predicted.

##Ridge and lasso Regression##

Both forms of penalized regression-ridge and lasso-struck us as potentially valuable in that the full model did pretty well on its own, thus a penalized regression might improve its accuracy by potentially reducing variance with only a small concomittant increase in bias. Due to the large number of predictors in the full model (30), we were quite concerned about variance thus we hoped that ridge and lasso would help.

```{r}
#Ridge Model for ERA
ridge_mod_ERA<- glmnet(x_full_model_ERA, y_full_model_ERA, alpha = 0, lambda = best_L_ridge_ERA)

#Ridge Model for Salary
ridge_mod<- glmnet(x_full_model_salary, y_full_model_salary, alpha = 0, lambda = best_L_ridge)

#Lasso Model for ERA
lasso_mod<- glmnet(x_full_model_salary, y_full_model_salary, alpha = 0, lambda = best_L_lasso)

#Lasso Model for Salary
lasso_mod_ERA<- glmnet(x_full_model_ERA, y_full_model_ERA, alpha = 1, lambda = best_L_lasso_ERA)

```

Above we see the 4 models, one for salary and one for ERA for each of the penalized regressions. The best lambda value for cost-complexity tuning was determined through cross-validation using a range of values.

###Ridge and Lasso Results##

```{r}
ridge_statistics<-c(0.62543602, 0.33715364, 0.09284193, 0.99001384)
lasso_statistics<-c(0.6299247, 0.3284793, 0.1072762, 0.9734646)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

penalized_reg_table<-data.frame(labels,ridge_statistics,lasso_statistics) 
penalized_reg_table<-penalized_reg_table %>%  rename(
    `Ridge Regression Statistics` = ridge_statistics,
    `Lasso Regression Statistics` = lasso_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels,
    )

kable(penalized_reg_table)

```

As seen above, both forms of penalized regression led to pretty significant reductions in 5-fold CV MSE for both ERA and salary but especially for salary. The CV MSE for salary was more than cut in half after applying the penalty to the full model. This suggests that the two methods achieved their desired function by trading off a small amount of bias for a larger decrease in variance.

Interestingly, lasso performed better than ridge for both future salary and ERA. What this suggests is that, perhaps due to a high degree of correlation among predictors, certain predictor coefficients can be set to 0 in the interest of predictive accuracy. Similar to best subet, lasso is performing variable selection which perhaps explains its increased effectiveness.

##Principal Component Regression##

Best subset offers us one solution against collinearity by dropping some of the correlated predictors. Principal component regression offers us another way as we can simply combine various predictors into new ones that adequately explain much of the variation in the data. By doing so, we can perhaps remedy some of the correlations among our predictors.

The number of principal components to use in each model was chosen based off crossed-validation. Interestingly, ERA only required 4 principal components while salary required 13. This could suggest that, while 4 principal components are sufficient to capture the variability in future ERA, more principal components are necessary to capture the variance in future salary.
```{r}
#PCR Model for ERA
my_pcr_ERA <- pcr(formula = ERA_t1 ~ ., ncomp=4,data = pitchers)

#PCR Model for Salary
my_pcr_salary <- pcr(formula = log(salary_t1) ~ ., ncomp=13, data = salary_data)
```

##PCR Results##
```{r}
PCR_statistics<-c(0.65489,0.3313,0.13856,0.9972)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

PCR_table<-data.frame(labels,PCR_statistics) 
PCR_table<-PCR_table %>%  rename(
    `Principal Component Regression Statistics` = PCR_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels,
    )

kable(PCR_table)

```

While principal component regression did lead to an improvement from the full model, it did not lead to as much of an improvement as we expected. While PCR performed at around the same level as ridge, lasso out-performed both of them. It is also notable that ridge, lasso, and PCR resulted in large reductions in salary 5-fold CV MSE but not for ERA, suggesting that the salary data benefits more from the various attempt at reducing collinearity.

##Custom Model##

For the custom model, we relied on a mixture of domain knowledge as well as stepwise selection. We began with the forward selection model which performed quite well and then removed some of the predictors we did not believe to be important such as losses (which we changed to games) as well as batters faced while adding different predictors and ultimately settiling on spin rate. We also included ERA*hard hit % and barrel %. The reason behind this is that we saw barrel and hard hit % as proxies for the amount of hard contact a pitcher gives up. Thus if a pitcher's ERA is low but their hard hit and barrel % are high, their low ERA is more a result of luck than their performance. Those two terms deal with that situation.

Still need a custom salary model.

```{r}
custom_mod <- lm(data = pitchers, 
                  ERA_t1 ~ spin_rate + G + SO + K_percent + 
                    ERA:hard_hit_percent + ERA:barrel_percent + luck_adj_ERA)

```

###Custom Model Results###
```{r}
domain_knowledge_statistics<-c(0.6429,0.3293462,0.1573634,0.972449)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

custom_table<-data.frame(labels,domain_knowledge_statistics) 
custom_table<-custom_table %>%  rename(
    `Custom Model Statistics` = domain_knowledge_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels,
    )

kable(custom_table)

```

As seen above, our custom model actually produced the lowest 5-fold MSE among the sampled models. This is somewhat understandable as we combined stepwise selection with domain knowledge to produce a model that intuitively "makes sense". This model is also less complex than the forward model, perhaps contributing to its reduced MSE as it is less susceptible to variance.

##Overall Results##

```{r}
ridge_statistics<-c(0.62543602, 0.33715364, 0.09284193, 0.99001384)
lasso_statistics<-c(0.6299247, 0.3284793, 0.1072762, 0.9734646)
forward_statistics<-c(0.6169164,0.7657645,0.1642511,0.9994591)
domain_knowledge_statistics<-c("NA","NA",0.1573634,0.972449)
full_statistics<-c(0.6092759,0.8313169,0.1397712,1.04342)
PCR_statistics<-c(0.65489,0.3313,0.13856,0.9972)
ensemble_statistics <-c(0.776,0.2209,0.149,0.970225)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

all_statistics<-data.frame(labels,ridge_statistics,lasso_statistics,forward_statistics,domain_knowledge_statistics,full_statistics,PCR_statistics, ensemble_statistics)

all_statistics<-all_statistics %>%  rename(
    `Full Model` =full_statistics,
    `Forward Model` = forward_statistics,
    `Ridge Regression`=ridge_statistics,
    `Lasso Regression`=lasso_statistics,
    `Custom Model`=domain_knowledge_statistics,
    `Principal Regression` = PCR_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels,
    )

kable(all_statistics)
```

```{r}
final_table <- tibble(
    Model = c("Ridge", "Lasso", "Forward","Custom","Full","PCR", "Ensemble"),
    `Rsq for ERA` = c(0.092,0.107,0.164,0.157,0.139,0.138,0.149),
    `5-fold MSE for ERA` = c(0.990,0.973,0.999,0.972,1.043, 0.997,0.970),
    `Rsq for Salary` = c(0.625,0.629,0.616,0.6429,0.609,0.654,0.776),
    `5-fold MSE for Salary` = c(0.337,0.328,0.765,0.3293462,0.831,0.331,0.220))

kable(final_table)
```

We can see that in terms of R^2 the forward model performed the best, and that in terms of 5-fold MSE the ensemble model performed the best. The 5-fold MSE difference might seem marginal, but we have to remember that the MSE is in `log(Salary)` units.

```{r}
ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`Rsq for ERA`)),
       aes(
           x = factor(Model),
           y = `Rsq for ERA`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(x = "Model") +
    expand_limits(x = 0, y = 0)

ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`5-fold MSE for ERA`)),
       aes(
           x = factor(Model),
           y = `5-fold MSE for ERA`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(x = "Model") +
    expand_limits(x = 0, y = 0)


```

For salary we can see that the ensemble model far outperforms the other models in terms of R^2. Similarly, for 5-fold MSE the ensemble model outperforms all other models.

```{r}
ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`Rsq for Salary`)),
       aes(
           x = factor(Model),
           y = `Rsq for Salary`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(x = "Model") +
    expand_limits(x = 0, y = 0)

ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`5-fold MSE for Salary`)),
       aes(
           x = factor(Model),
           y = `5-fold MSE for Salary`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(x = "Model") +
    expand_limits(x = 0, y = 0)

```

#Conclusions#

We have split our conclusions into two parts, predictive and inferential. The reason for this was that the more complex ensemble models that excelled in prediction are not as amenable to interpretation as some simpler models such as the custom linear regression models or the lasso penalized regression.

##Inferential Conclusions##
```{r}
custom_mod <- lm(data = pitchers, 
                  ERA_t1 ~ spin_rate + G + SO + K_percent + 
                    ERA:hard_hit_percent + ERA:barrel_percent + luck_adj_ERA)

custom_mod_salary <- lm(data = salary_data,
                         log(salary_t1)~Year+SO+
                           Salary+W*luck_adj_ERA)
```

Above, we see the two linear regressions we created for our custom models, which was built using domain knowledge to create initial multiple linear regressions, and then performing manual stepwise selection by dropping insignificant predictors and adding new ones. 

In broad strokes, they both are pretty simple. The ERA model has 7 total predictors, and the salary model has 5 total predictors including an interaction term. Our hypothesis as to why the simpler models outperformed the full model is due to a high amount of correlation between predictors. In our data, the volume-correlated statistics such as total wins, total pitches, total strikeouts, were highly correlated and the advanced statistics such as xwOBA, xBA and BABIP were also correlated with each other as well. These simple models circumvent this issue by only using some of the correlated predictors and dropping the rest.

When it comes to performing interpretation on the models, we looked at both the predictors that were included as well as the significance (in p-value) of each predictor. First, looking at the ERA model, in terms of predictor inclusion, we see a mix of counting statistics such as games and strikeouts as well as advanced statistics such as spin rate, barrel %, and luck-adjusted ERA. Of the 7 variables, 2 of them fall into the counting statistics category, strikeout percent is unique in that it is a slightly processed form of a basic counting statistic, while the other 4 are closer to advanced statistics, suggesting that statistics-driven analytics might be more helpful in performing predictions of future ERA. Then, looking at significance, ERA*barrel % stands out as having the smallest p-value. As previously discussed, this term attempts to remove some of the noise from the base ERA metric. The positive coefficient makes sense as higher ERA or higher barrel % are both likely to lead to a higher ERA next year. The next smallest p-value belongs to luck-adjusted ERA, which penalizes “lucky” pitchers with a regression to their mean. The other two predictors with low p-values were the related predictors of total strikeouts and strikeout rates, with strikeout rate having a lower p-value. The inclusion of the basic counting statistic of total strikeout as well as the minimally processed statistics of strikeout rates suggest that, while advanced analytics are useful, counting statistics still have an important role as well. Overall, even though the ERA prediction model integrates both counting statistics and advanced statistics, it seems like it relies quite heavily upon advanced analytics, even though basic metrics still play a role.

Then, looking at the salary model, we can see that the 5 included predictors consist mostly of basic metrics such as total strikeouts, previous salary, total wins, and even the year. The one included metric that utilized advanced statistics was the luck adjusted ERA, which uses Sabermetric statistics such as xwOBA to help quantify luck. Already, we can see a pretty significant difference from the ERA model in terms of predictors chosen. Unsurprisingly, the most important predictor of future salary was past salary, as quantified through comparison of p-values. However, to our surprise, year was the only other significant (p-value<0.05) predictor suggesting that the different economic climates of the league in different years plays an important role. Even more shockingly, the coefficient was negative, while we had expected a positive relationship suggesting inflation, the negative coefficient rather suggests that teams are paying less and less for pitchers. Other predictors of note with low p-values were total strikeouts as well as luck adjusted ERA. The positive coefficient on total strikeouts and the negative relationship with ERA makes sense as both increasing strikeouts and decreasing ERA are indicators of success and thus should be compensated as such.

Lastly, we can compare the predictors included in each as well as their associated significance. Just from the basis of included predictors, we can already see a difference in terms of representation of advanced analytics. While the majority of predictors used in predicting ERA were advanced statistics such as hard-hit percent or barrel percent, those Sabermetric statistics don’t appear in the salary model, save for luck adjusted ERA. What this suggests is that front offices have perhaps not fully integrated Sabermetrics into their contract discussions. In fact, luck adjusted ERA can be switched out for ERA without any decrease in model accuracy for salary, suggesting that front offices have yet to embrace luck reversion.
