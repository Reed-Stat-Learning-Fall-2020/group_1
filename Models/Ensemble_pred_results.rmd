---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(stacks)
```

## Predictive Conclusions for the Ensemble model

We saw a clear winner in terms of predictive accuracy with our Ensemble model. In the end it was just made up of a lasso regression model and random forest model. We avoided using models like boosted trees and ridge regression because of how similar they are to the two that we used.

```{r}
ensemble_results <- read_csv("Models/ensemblepreds.csv") %>%
  select(.pred, salary_t1, Year, Pitcher)
```

We can see that overall our ensemble model does a pretty decent job in terms of it's predictive accuracy. Additionally it looks like our model had a pretty difficult time predicting salaries that were on the lower end of things, it tended to overpredict them pretty consistently.

```{r}
ensemble_results %>%
  ggplot() +
  aes(
    x = salary_t1, 
    y = .pred
  ) +
  geom_point() + 
  coord_obs_pred() +
  geom_abline(slope = 1, intercept = 0, color = "midnightblue", alpha = 0.5, size = 2) +
  labs(x = "logged Salary (t+1)", y = "Prediction") +
  theme_minimal()

```

Lets take a look to see if that discrepancy is consistent for each year

```{r}
ensemble_results %>%
  ggplot() +
  aes(
    x = salary_t1, 
    y = .pred
  ) +
  geom_point() + 
  coord_obs_pred() +
  geom_abline(slope = 1, intercept = 0, color = "midnightblue", alpha = 0.5, size = 2) +
  facet_wrap(~Year) +
  theme_minimal() +
  labs(x = "Salary (t+1)", y = "predictions")
```

It seems like we likely don't quite have enough data to say anything too conclusive about this, but it does seem like most of the problems occured in 2019 and there were just a couple of big misses in 2015 and 2018.

Overall we're a bit unsurprised by the fact that the ensemble model performed so well because it was essentially a combination of two of our other successful models. We were able explain a good amount of the variation in the next year's salary and out test MSE was impressively better than any other model.

Here's another way to visualize how we did. We can see that the occasions when our model did the worst was when we overpredicted a salary. In other words we see that same struggle with predicting low salaries here.

```{r}
ensemble_results %>%
  mutate(diff = salary_t1 - .pred) %>%
  distinct(Pitcher, .keep_all = T) %>%
  mutate(Pitcher =  fct_reorder(Pitcher, abs(diff))) %>%
  ggplot(aes(x = Pitcher, y = abs(diff), color = diff < 0)) +
  geom_point() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45)) +
  labs(y = "Residuals")

```


