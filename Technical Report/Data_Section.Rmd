---
title: "Final Data Section"
author: "Group 1-Josh Yamamoto, Riley Leonard, Andy Zhao"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(kableExtra)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
pitchers0 <- read_csv("Final Mega Summary.csv")

salary_data <- pitchers0 %>%
  select(-Pitcher, -ΔERA, -ERA_t1, -ΔSalary)

pitchers <- pitchers0 %>%
  select(-Pitcher, -ΔERA,- salary_t1, -ΔSalary)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
pitchers_2019 <- pitchers0 %>%
  filter(Year == 2019) %>%
  arrange(desc(salary_t1)) %>%
  head(50) %>%
  select(Pitcher, Year, W, L, G, ERA, spin_rate, SO, BB, K_percent, 
         hard_hit_percent, barrel_percent, luck_adj_ERA, ERA_t1, salary_t1)

custom_mod <- lm(data = pitchers, 
                  ERA_t1 ~ spin_rate + G + SO + K_percent + 
                    ERA:hard_hit_percent + ERA:barrel_percent + luck_adj_ERA)

custom_pred_2019 <- predict(custom_mod, pitchers_2019, type = "response")

custom_pred <- predict(custom_mod, pitchers0, type = "response")

pitchers_2019 <- cbind(pitchers_2019, custom_pred_2019)

pitchers0 <- cbind(pitchers0, custom_pred)

pitchers_2019 <- pitchers_2019 %>%
  mutate(`Standardized Forecasted Adjusted ERA` = -scale(custom_pred_2019)) %>%
  mutate(`Standardized Salary` = scale(salary_t1)) %>%
  mutate(`Compensation` = ((salary_t1)/(10 -custom_pred_2019))/100000)

pitchers_2019 <- pitchers_2019 %>%
  mutate(`Standardized Forecasted Adjusted ERA` = format(round(`Standardized Forecasted Adjusted ERA`, 2), nsmall = 2)) %>%
  mutate(`Standardized Salary` = format(round(`Standardized Salary`, 2), nsmall = 2))

pitchers_2019 <- pitchers_2019 %>%
  mutate(`Standardized Forecasted Adjusted ERA` = as.numeric(`Standardized Forecasted Adjusted ERA`)) %>%
  mutate(`Standardized Salary` = as.numeric(`Standardized Salary`))

pitchers_2019$ERA_type <- ifelse(pitchers_2019$`Standardized Forecasted Adjusted ERA` < 0, "below", "above")

pitchers_2019$salary_type <- ifelse(pitchers_2019$`Standardized Salary` < 0, "below", "above")

pitchers_2019_tidy <- pitchers_2019 %>%
  pivot_longer(cols = c(custom_pred_2019, ERA_t1), 
               names_to = "type",
               values_to = "value")

salary_2019 <- pitchers0 %>%
  filter(Year == 2019) %>%
  arrange(desc(salary_t1)) %>%
  head(50)

custom_mod_salary <- lm(data = salary_data,
                         log(salary_t1) ~ Year + SO +
                           Salary + W*luck_adj_ERA)

salary_pred_2019 <- predict(custom_mod_salary, salary_2019, type = "response")

salary_2019 <- cbind(salary_2019, salary_pred_2019)

salary_2019_tidy <- salary_2019 %>%
  mutate(salary_t1 = log(salary_t1)) %>%
  pivot_longer(cols = c(salary_pred_2019, salary_t1), 
               names_to = "type",
               values_to = "value") 
```

## Data and Results

After describing some of the trends we observed in the data, we then turn towards the variety of models that we have created, as well as their respective performances, measured in adjusted R-squared as well as 5-fold cross-validation mean-squared error (MSE).




### Full Model

```{r eval=FALSE}
# Full Model for ERA

full_mod <- lm(data = pitchers,
                 ERA_t1 ~.)

# Full Model for Salary

full_mod <- lm(data = salary_data,
                 salary_t1 ~.)
```



Above, we see the code for the full model on both salary and ERA. We expected that the full model would do adequetely, as we had hand-selected every variable included in our dataset and thus we expected all of our variables to be at least somewhat significant. However, one of the issues we had foresaw was the presence of a great deal of collinearity, which could make our parameter estimates quite unstable, and the full model lacks the sufficient means to counter said collinearity.




#### Full Model Results

```{r echo=FALSE}
full_statistics<-c(0.6092759,0.8313169,0.1397712,1.04342)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

full_model_table<-data.frame(labels,full_statistics) 
full_model_table<-full_model_table %>%  rename(
    `Full Model Statistics` = full_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels)

kable(full_model_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```


It appears that our full model struggles with the data compared to our other models. This is understandable for the reasons listed above, namely the presence of collinearity between predictors. Interestingly, even the full model has a 5-fold CV MSE for ERA above that of the null model of SIERA (see introduction for a full discussion of the null model), suggesting that even the weakest full model still outperformed existing ERA projection methodologies. It is also notable that the adjusted R-squared for ERA is far weaker than that of salary. This is understandable, as the irreducible error in the ERA data far exceeds that of the salary data. While there is some stochasticity in contract negotiations, there is far more noise when it comes to on-field performance.




### Forward Selection

```{r eval=FALSE}
# Forward Selection Model for ERA

forward_mod <- lm(data = pitchers, 
                   ERA_t1 ~ wOBA + L + BFP + SO + K_percent + hard_hit_percent +
                    barrel_percent + `ERA/Barrel %` + luck_adj_ERA)

# Forward Selection Model for Salary

forward_mod_salary <- lm(data = salary_data,
                         log(salary_t1) ~ Pitches + Year + xBA + spin_rate +
                           Velocity + K_percent + Salary + luck_adj_ERA)
```



We then moved on to forward selection, which is an efficient alternative to best subset. We set the maximum complexity of the model (nvmax) to be considered at 12. We chose this because of the aforementioned problems with collinearity. Subset selection gives us a way to reduce collinearity by simply dropping some of the correlated predictors. We set the maximum to 12 because we wanted a relatively parsimonious model. Ultimately, the adjusted R-squared of the model with 9 predictors performed the best among all the 12 ERA models, while the 8-predictor model performed the best for salary.

While a more detailed discussion will be left for the conclusion, in broad strokes, the predictors included in the two models are not as different as one would expect. The presence of expected batting averaged (xBA) and spin rate in the salary model suggests that these sabermetric statistics actually result in a larger contract, but, conversely, base counting statistics like total pitches and even year is also included.

The forward model for ERA includes a number of advanced statistics such as wOBA, hard hit %, barrel %, and even ERA/barrel %. However, it also includes counting statistics like total strikeouts and total batters faced. Ultimately, the forward model for the two are more similar than one would have intuited. 



#### Forward Selection Results

```{r echo=FALSE}
forward_statistics<-c(0.6169164,0.7657645,0.1642511,0.9994591)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

forward_model_table<-data.frame(labels,forward_statistics) 
forward_model_table<-forward_model_table %>%  rename(
    `Forward Model Statistics` = forward_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels)

kable(forward_model_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```


The forward selection method of subset selection led to improvements over the full model, likely due to reduction in collinearity by dropping some of the correlated predictors. Interestingly, the improvement is quite marginal and did not lead to as major of an increase in 5-fold CV MSE or adjusted R-squared as one would have predicted.




### Ridge and LASSO Regression

Both forms of penalized regression---ridge and LASSO---struck us as potentially valuable in that the full model performed fairly well on its own, thus a penalized regression might improve its accuracy by potentially reducing variance with only a small concomittant increase in bias. Due to the large number of predictors in the full model (30), we were quite concerned about variance and thus hoped that ridge and LASSO would help.


```{r eval=FALSE}
# Ridge Model for ERA

ridge_mod_ERA <- glmnet(x_full_model_ERA, 
                       y_full_model_ERA, 
                       alpha = 0, 
                       lambda = best_L_ridge_ERA)

# Ridge Model for Salary

ridge_mod <- glmnet(x_full_model_salary, 
                   y_full_model_salary, 
                   alpha = 0, 
                   lambda = best_L_ridge)

# Lasso Model for ERA

lasso_mod <- glmnet(x_full_model_salary, 
                    y_full_model_salary, 
                    alpha = 1, 
                    lambda = best_L_lasso)

# Lasso Model for Salary

lasso_mod_ERA <- glmnet(x_full_model_ERA, 
                        y_full_model_ERA, 
                        alpha = 1, 
                        lambda = best_L_lasso_ERA)

```



Above, we see the 4 models, one for salary and one for ERA for each of the penalized regressions. The best lambda value for cost-complexity tuning was determined through cross-validation using a range of values.



#### Ridge and LASSO Results

```{r echo=FALSE}
ridge_statistics<-c(0.62543602, 0.33715364, 0.09284193, 0.99001384)
lasso_statistics<-c(0.6299247, 0.3284793, 0.1072762, 0.9734646)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

penalized_reg_table<-data.frame(labels,ridge_statistics,lasso_statistics) 
penalized_reg_table<-penalized_reg_table %>%  rename(
    `Ridge Regression Statistics` = ridge_statistics,
    `Lasso Regression Statistics` = lasso_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels)

kable(penalized_reg_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```


As we see, both forms of penalized regression led to pretty significant reductions in 5-fold CV MSE for both ERA and salary, but especially for salary. The CV MSE for salary was more than cut in half after applying the penalty to the full model. This suggests that the two methods achieved their desired function by trading off a small amount of bias for a larger decrease in variance.

Interestingly, LASSO performed better than ridge for both future salary and ERA. What this suggests is that, perhaps due to a high degree of correlation among predictors, certain predictor coefficients can be set to 0 in the interest of predictive accuracy. Similar to best subet, LASSO is performing variable selection (which perhaps explains its increased effectiveness).




### Principal Component Regression

Best subset offers us one solution against collinearity by dropping some of the correlated predictors. Principal component regression offers us another method by combining various predictors into new ones that adequately explain much of the variation in the data. By doing so, we can perhaps remedy some of the correlations among our predictors.

The number of principal components to use in each model was chosen based off crossed-validation. Interestingly, ERA only required 4 principal components, while salary required 13. This could suggest that, while 4 principal components are sufficient to capture the variability in future ERA, more principal components are necessary to capture the variance in future salary.



```{r eval=FALSE}
# PCR Model for ERA

my_pcr_ERA <- pcr(formula = ERA_t1 ~ ., 
                  ncomp = 4,
                  data = pitchers)

# PCR Model for Salary

my_pcr_salary <- pcr(formula = log(salary_t1) ~ ., 
                     ncomp = 13, 
                     data = salary_data)

```




#### PCR Results

```{r echo=FALSE}
PCR_statistics<-c(0.65489,0.3313,0.13856,0.9972)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

PCR_table<-data.frame(labels,PCR_statistics) 
PCR_table<-PCR_table %>%  rename(
    `Principal Component Regression Statistics` = PCR_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels)

kable(PCR_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```


While principal component regression did lead to an improvement from the full model, it did not lead to as much of an improvement as we expected. While PCR performed at around the same level as ridge, LASSO out-performed both ridge and PCR. It is also notable that ridge, LASSO, and PCR resulted in large reductions in salary 5-fold CV MSE but not for ERA, suggesting that the salary data benefits more from the various attempts at reducing collinearity.




### Custom Model

To construct a custom model, we relied on an amalgam of domain knowledge, variable interaction, and stepwise selection. We began with the aforementioned forward selection model, and then removed some of the predictors we did not believe to be important based on both underlyting domain knowledge and the reported significance of the variable coefficients in the model summary. The variables we chose to remove included games and batters faced. We then added spin rate as a predictor. While spin rate had very little effect on the change in response in the multiple linear regression (with a slope coeffcient estimate equal to approximately), it was nonetheless included in our model due to its sttatistcial significance and model-stabilizing effect. We then included two interaction terms, first between ERA and hard hit %, and the ERA and barrel %. These two interaction effects, while seemingly redudant with the already included luck-adjusted metrics, managed to dramatically improve overall model fit, and were therefore included in the final model.



```{r eval=FALSE}
# Custom Model for ERA

custom_mod <- lm(data = pitchers, 
                  ERA_t1 ~ spin_rate + G + SO + K_percent + 
                    ERA:hard_hit_percent + ERA:barrel_percent + luck_adj_ERA)

```




#### Custom Model Results

```{r echo=FALSE}
domain_knowledge_statistics<-c(0.6429,0.3293462,0.1573634,0.972449)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

custom_table<-data.frame(labels,domain_knowledge_statistics) 
custom_table<-custom_table %>%  rename(
    `Custom Model Statistics` = domain_knowledge_statistics,
    `Adjusted R-squared and 5-Fold CV MSE` = labels)

kable(custom_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)

```


As seen above, our custom model actually produced the lowest 5-fold MSE among the sampled models. This is somewhat understandable, as we combined stepwise selection with domain knowledge to produce a model build around intuition. This model is also less complex than the forward model, perhaps contributing to its reduced MSE as it is less susceptible to variance.




### Overall Results

```{r echo=FALSE}
ridge_statistics<-c(0.62543602, 0.33715364, 0.09284193, 0.99001384)
lasso_statistics<-c(0.6299247, 0.3284793, 0.1072762, 0.9734646)
forward_statistics<-c(0.6169164,0.7657645,0.1642511,0.9994591)
domain_knowledge_statistics<-c("NA","NA",0.157,0.972)
full_statistics<-c(0.6092759,0.8313169,0.1397712,1.04342)
PCR_statistics<-c(0.65489,0.3313,0.13856,0.9972)
ensemble_statistics <-c(0.776,0.2209,0.149,0.970225)
labels<-c("R-Squared on Salary", "5-fold MSE on Salary","R-Squared on ERA","5-Fold MSE on ERA")

all_statistics<-data.frame(labels,ridge_statistics,lasso_statistics,forward_statistics,domain_knowledge_statistics,full_statistics,PCR_statistics, ensemble_statistics)

all_statistics<-all_statistics %>%  rename(
    `Full Model` =full_statistics,
    `Forward Model` = forward_statistics,
    `Ridge Regression`=ridge_statistics,
    `Lasso Regression`=lasso_statistics,
    `Custom Model`= domain_knowledge_statistics,
    `Principal Regression` = PCR_statistics,
    `Ensemble Model` = ensemble_statistics,
    ` ` = labels)

kable(all_statistics, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```

```{r echo=FALSE}
final_table <- tibble(
    Model = c("Ridge", "Lasso", "Forward","Custom","Full","PCR", "Ensemble"),
    `Rsq for ERA` = c(0.092,0.107,0.164,0.157,0.139,0.138,0.149),
    `5-fold MSE for ERA` = c(0.990,0.973,0.999,0.972,1.043, 0.997,0.970),
    `Rsq for Salary` = c(0.625,0.629,0.616,0.6429,0.609,0.654,0.829),
    `5-fold MSE for Salary` = c(0.337,0.328,0.765,0.3293462,0.831,0.331,0.181))

kable(final_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```



We can see that in terms of R-squared, the forward model performed the best. Conversely, in terms of 5-fold MSE, the ensemble model performed the best. The 5-fold MSE difference might seem marginal, but we have to remember that the MSE is in `log(Salary)` units.



```{r echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`Rsq for ERA`)),
       aes(
           x = factor(Model),
           y = `Rsq for ERA`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(title = "ERA Model Comparisons",
         x = "Model",
         y = "R-Squared for ERA") +
    expand_limits(x = 0, y = 0)

ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`5-fold MSE for ERA`)),
       aes(
           x = factor(Model),
           y = `5-fold MSE for ERA`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(title = "Salary Model Comparisons",
         x = "Model") +
    expand_limits(x = 0, y = 0)


```



For salary, we can see that the ensemble model far outperforms the other models in terms of R-squared. Similarly, in terms of 5-fold MSE, we see that the ensemble model again outperforms all other models:



```{r echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`Rsq for Salary`)),
       aes(
           x = factor(Model),
           y = `Rsq for Salary`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(title = "ERA Model Comparisons",
         x = "Model",
         y = "R-Squared for ERA") +
    expand_limits(x = 0, y = 0)

ggplot(final_table %>%
           mutate(Model = fct_reorder(Model,`5-fold MSE for Salary`)),
       aes(
           x = factor(Model),
           y = `5-fold MSE for Salary`,
           group = 1)
       ) +
    stat_summary(
        fun.y=sum,
        geom="line",
        color = "midnightblue",
        size = 2.5, 
        alpha = 0.6
        ) +
    geom_point(
        color = "white",
        size = 1.5
        ) +
    theme_minimal() +
    labs(title = "Salary Model Comparisons",
         x = "Model") +
    expand_limits(x = 0, y = 0)

```




## Conclusions

We have split our conclusions into two parts, predictive and inferential. The reason for this was that the more complex ensemble models that excelled in prediction are not as amenable to interpretation as some simpler models (such as the custom linear regression models or the lasso penalized regression).




### Inferential Conclusions

```{r}
# Comparing Custom Models

custom_mod <- lm(data = pitchers, 
                  ERA_t1 ~ spin_rate + G + SO + K_percent + 
                    ERA:hard_hit_percent + ERA:barrel_percent + luck_adj_ERA)

custom_mod_salary <- lm(data = salary_data,
                         log(salary_t1) ~ Year + SO +
                           Salary + W*luck_adj_ERA)

```


Above, we see the two linear regressions we created for our custom models, which were built using domain knowledge to create initial multiple linear regressions, and then were fine-tuned by introducing interaction terms and performing manual stepwise selection by dropping insignificant predictors and adding new ones. 

In broad strokes, they both are pretty simple. The ERA model has 7 total predictors, and the salary model has 5 total predictors (including an interaction term). Our hypothesis as to why the simpler models outperformed the full model is due to a high amount of correlation between predictors. In our data, the volume-correlated statistics such as total wins, total pitches and total strikeouts were highly correlated. Similarly, the advanced statistics such as xwOBA, xBA, and BABIP were also correlated with each other as well. These simple models circumvent this issue by only using some of the correlated predictors and dropping the rest.


```{r, echo=FALSE}
kable(summary(custom_mod)$coef, digits = 3,
      caption = "Custom ERA Model Coeffcients") %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```

```{r, echo=FALSE}
kable(summary(custom_mod_salary)$coef, digits = 3,
      caption = "Custom Salary Model Coeffcients") %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```



When it comes to performing interpretation on the models, we looked at both the predictors that were included as well as the significance (in p-value) of each predictor. First, looking at the ERA model, in terms of predictor inclusion, we see a mix of counting statistics such as games and strikeouts as well as advanced statistics such as spin rate, barrel %, and luck-adjusted ERA. Of the 7 variables, 2 of them fall into the counting statistics category, strikeout percent is unique in that it is a slightly processed form of a basic counting statistic, while the other 4 are closer to advanced statistics, suggesting that statistics-driven analytics might be more helpful in performing predictions of future ERA. Then, looking at significance, the interaction term bewteen ERA and barrel % stands out as having the smallest p-value. As previously discussed, this term attempts to remove some of the noise from the base ERA metric. The positive coefficient makes sense, as a higher ERA or higher barrel % is likely to lead to a higher ERA in the succeeding year. The next smallest p-value belongs to luck-adjusted ERA, which penalizes “lucky” pitchers with a regression to the mean of an aggregation of luck statistics. The other two predictors with low p-values were the related predictors of total strikeouts and strikeout rates, with strikeout rate having a lower p-value. The inclusion of the basic counting statistic of total strikeout as well as the minimally processed statistics of strikeout rates suggest that, while advanced analytics are most useful in the deternimation of future ERA, counting statistics still have an important role as well. Overall, even though the ERA prediction model integrates both counting statistics and advanced statistics, it seems like it relies quite heavily upon advanced analytics, even though basic metrics still play a role.

Conversely, looking at the salary model, we see that the 5 included predictors consist mostly of basic metrics such as total strikeouts, previous salary, total wins, and even the year. The one included metric that utilized advanced statistics was the luck adjusted ERA, which uses Sabermetric statistics such as xwOBA to help quantify luck. Already, we can see a pretty significant difference from the ERA model in terms of predictors chosen. Unsurprisingly, the most important predictor of future salary was past salary, as quantified through comparison of p-values. However, to our surprise, year was the only other significant ($p-value < 0.05$) predictor, suggesting that the different economic climates of the MLB in different years plays an important role in determining future salary. Even more shockingly, the coefficient was negative. While we had expected a positive relationship (suggesting inflation), the negative coefficient suggests that teams are actually paying less and less for pitchers in successive years. Other predictors of note with low p-values were total strikeouts and luck adjusted ERA. The positive coefficient on total strikeouts and the negative relationship with ERA make sense, as both increasing strikeouts and decreasing ERA are indicators of success and should be compensated as such.

Lastly, we can compare the predictors included in each model, as well as their associated significance. Just from the basis of included predictors, we can already see a difference in terms of representation of advanced analytics. While the majority of predictors used in predicting ERA were advanced statistics such as hard-hit percent or barrel percent, those Sabermetric statistics don’t appear in the salary model, save for luck adjusted ERA. What this suggests is that front offices have perhaps not fully integrated Sabermetrics into their contract discussions. In fact, luck adjusted ERA can be switched out for ERA without any decrease in model accuracy for salary, suggesting that front offices have yet to discover or embrace the statistical concept of luck reversion.




### Predictive Conclusions

In addition to using inference to assess the discrepancies in the patterns that determine future performance and future salary, we can also utilize the predictive powers of our models to make additional evaluations. In the interest of illustrating the predictive power of our custom ERA model, the first step we took was visualizing and measuring prediction accuracy of our model against the null (ERA-only) model:

```{r echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(pitchers,
       aes(x = custom_pred, y = ERA_t1)) +
  geom_point(alpha = 0.5, color = "midnightblue") +
  geom_smooth(method = lm, color = "midnightblue") +
  labs(title =  "Predicted ERA vs. Actual ERA (2016-2020)",
       subtitle = "Modelling Predictions",
       x = "Predicted ERA (t+1)",
       y = "ERA (t+1)") + 
  theme_classic()
```

```{r echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(pitchers,
       aes(x = ERA, y = ERA_t1)) +
  geom_point(alpha = 0.5, color = "midnightblue") +
  geom_smooth(method = lm, color = "midnightblue") +
  labs(title =  "Predicted ERA vs. Actual ERA (2016-2020)",
       subtitle = "Null Predictions",
       x = "ERA",
       y = "ERA (t+1)") + 
  theme_classic()
```


```{r echo=FALSE}
cor_table <- tibble(
    Model = c("Custom ERA Model", "Null ERA Model"),
    `Correlation (R)` = c(0.413, 0.218))

kable(cor_table, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", font_size = 12, full_width = FALSE)
```


As we can see from the above visualizations, our custom model for forecasting ERA comfortably outperforms the null model. This is also evident in the two models' respective correlation coeffiecients, which measure the correlation between the actual values and predicted values for each model. In both cases, we can discern the difficulty in accurately forecasting ERA, as even our best performing model struggles to avoid some degree of prediction error. This is to be expected though, as predicting patterns of human performance (such as ERA) is a reasonably difficult task, especially when exposed to a large amount of stochastic error. That being said, our model is still accurate enough to make it useful in application. One such application is to compare the predicted ERA and actual ERA of pitchers in a given season, effectively measuring how well a pitcher performed relative to the modelling expectations determined by the previous year's performance.



```{r echo=FALSE, fig.width=9,fig.height=6}
ggplot(pitchers_2019_tidy, aes(x = Pitcher,
                          y = value,
                          label = value)) + 
  geom_point(stat = 'identity',
             aes(color = type), 
             size = 2.5) +
  scale_color_manual(name = "Value", 
                    labels = c("Predicted ERA", "Actual ERA"), 
                    values = c("custom_pred_2019" = "deepskyblue", 
                               "ERA_t1" = "palevioletred2")) +
  scale_x_discrete(guide = guide_axis(angle = 50)) +
  labs(title =  "Predicted ERA vs. Actual ERA (2020)",
       x = "Pitcher",
       y = "ERA Value") + 
  geom_line() +
  theme_minimal()
```



From the above plot, which graphs predicted ERA (as determined by our ERA forecasting model) and actual ERA for the 50 highest-paid starting pitchers, we can ascertain which pitchers either overperformed or underperformed relative to modelling expectations. Specifically, we can look at the absolute difference between predicted ERA and actual ERA as a measure of expectation deviation. By this account, Anibal Sanchez, Jon Gray, Madison Bumgarner, Robbie Ray, and Tanner Roark stand out as pitchers who underperformed relative to their predicted performance. On the other hand, Dallas Keuchel, Shane Bieber, Trevor Bauer, and Zach Davies posted actual ERAs that exceeded the model's expectations. Implicitly, the model gives tends more conservative ERA prediction estimates, since the data used in linear model construction is necessarilly regressed. 

Another extension of our ERA model is using salary data to determine which pitchers were overpaid or underpaid relative to both their contemporaries and their expected pitching outcomes. Intuitively, front offices should spend money on pitchers who they believe will improve the overall pitching poerformance of their team in coming seasons. Subsequently, general managers should be interested in compensating players based on their expected future performance, since that will determine the return the team gets on their investment. It is therefore appropriate to assess a player's salary relative to their forecasted performance in order to determine whether or not that player is appropriately compensated, which we attempted using our ERA model and salary data. To allow for ease of comparison, we first standardized each pitcher's salary and forecasted ERA. We then created the following visualizations, each of which offer some sort of display comparing the standardized forecasted ERA and standardized salaries for the 50 highest-paid starting pitchers in the year 2020:


```{r, echo=FALSE, fig.width=7,fig.height=6}
ggplot(pitchers_2019, aes(x = reorder(Pitcher, `Standardized Forecasted Adjusted ERA`),
                          y = `Standardized Forecasted Adjusted ERA`,
                          label = `Standardized Forecasted Adjusted ERA`)) + 
  geom_bar(stat = 'identity', aes(fill = ERA_type), width = .5)  +
  scale_fill_manual(name = " ", 
                    labels = c("Above Average", "Below Average"), 
                    values = c("above"="deepskyblue", "below"="palevioletred2")) + 
  labs(title = "Diverging Standardized Forecasted ERA (2020)",
       x = "Pitcher",
       y = "Standardized ERA Forecast (Adjusted)") + 
  coord_flip() +
  theme_minimal()
```

```{r, echo=FALSE, fig.width=7,fig.height=6}
ggplot(pitchers_2019, aes(x = reorder(Pitcher, `Standardized Salary`),
                          y = `Standardized Salary`,
                          label = `Standardized Salary`)) + 
  geom_bar(stat = 'identity', aes(fill = salary_type), width = .5)  +
  scale_fill_manual(name = " ", 
                    labels = c("Above Average", "Below Average"), 
                    values = c("above"="deepskyblue", "below"="palevioletred2")) + 
  labs(title = "Diverging Standardized Salary (2020)",
       x = "Pitcher",
       y = "Standardized Salary") + 
  coord_flip() +
  theme_minimal()
```

```{r echo=FALSE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot() +
  scale_x_continuous(name = "Standardized Forecasted ERA (adjusted)",
                     breaks = seq(-3, 3, 1),
                     limits =c(-3, 3)) +
  scale_y_continuous(name = "Standardized Salary",
                     breaks = seq(-3, 3, 1),
                     limits =c(-3, 3)) +
  ggtitle("Standardized Forecasted ERA vs. Standardized Salary (2020)") +
  geom_rect(aes(xmin = 0, ymin = 0, xmax = 3, ymax = 3), 
            fill = "deepskyblue", alpha = 0.2) +
  geom_rect(aes(xmin = -3, ymin = -3, xmax = 0, ymax = 0), 
            fill = "palevioletred2", alpha = 0.2) +
  geom_density_2d(data = pitchers_2019, 
             aes(x = `Standardized Forecasted Adjusted ERA`, 
                 y = `Standardized Salary`),
             color = "dodgerblue2", alpha = 0.3) +
  geom_point(data = pitchers_2019, 
             aes(x = `Standardized Forecasted Adjusted ERA`, 
                 y = `Standardized Salary`),
             shape = 21,
             colour = "dodgerblue", 
             fill = "grey100", 
             size = 5) +
  geom_text(data = subset(pitchers_2019, 
                          `Standardized Forecasted Adjusted ERA` > 1 | 
                          `Standardized Salary` > 1),
            aes(x = `Standardized Forecasted Adjusted ERA` , 
                y = `Standardized Salary`, 
                label = Pitcher),
            size = 3,
            alpha = 0.8,
            hjust = 0.3,
            vjust = -1.2) + 
  geom_text(data = subset(pitchers_2019, 
                          `Standardized Forecasted Adjusted ERA` < -2),
            aes(x = `Standardized Forecasted Adjusted ERA` , 
                y = `Standardized Salary`, 
                label = Pitcher),
            size = 3,
            alpha = 0.8,
            hjust = 0.3,
            vjust = -1.33) + 
  theme_minimal()
```



Already, we can identify several players who appear to be appropriately paid for their forecasted performance. Gerrit Cole, Max Scherzer, Jacob DeGrom, and Yu Darvish are four pronounced examples, as each pitcher possesses both one of the highest standardized salaries and highest standardized forecasted ERAs (for this analysis, forecasted ERA has been adjusted such that a positive value is now associated with stronger preformances). On the other hand, we also notice players who represent examples of inneffieint decision making by front offices. Zack Greinke, who has one of the highest salaries of any starting pitcher in 2020, actually had a forecasted ERA below the mean, implying that teams could have made better use of that salary capital. On the contrary, starting pitchers Blake Snell and Mike Clevinger were modelled to have  two of the best expected ERAs, but both had salaries lower than the mean value. Lastly, in order to provide one convenient measure of pitcher compensation, we plotted salary over forecasted ERA for each of the 50 highest-paid starting pitchers for the year 2020, with higher values representing pitchers who were more genereously compensated for their forecasted performances:


```{r echo=FALSE, fig.width=9,fig.height=6}
ggplot(pitchers_2019, aes(x = reorder(Pitcher, Compensation),
                          y = Compensation,
                          label = Compensation)) + 
  geom_col(aes(fill = Compensation)) +
  scale_fill_continuous(type = "viridis") +
  scale_x_discrete(guide = guide_axis(angle = 50)) +
  labs(title =  "Starting Pitcher Compensation (2020)",
       x = "Pitcher",
       y = "Relative Compensation (Salary/Adjusted Forecasted ERA)") + 
  theme_minimal()
```



We can aslo use our salary model to measure which pitchers were underpaid relative to the existing standards and patterns of compensation identified by our model. 


```{r echo=FALSE, fig.width=9,fig.height=6}
ggplot(salary_2019_tidy, aes(x = Pitcher,
                          y = value,
                          label = value)) + 
  geom_point(stat = 'identity',
             aes(color = type), 
             size = 2.5) +
  scale_color_manual(name = "Value", 
                    labels = c("Predicted Salary", "Actual Salary"), 
                    values = c("salary_pred_2019" = "deepskyblue", 
                               "salary_t1" = "palevioletred2")) +
  scale_x_discrete(guide = guide_axis(angle = 50)) +
  labs(title =  "Predicted Salary vs. Actual Salary (2020)",
       x = "Pitcher",
       y = "Log (Salary)") + 
  geom_line() +
  theme_minimal()
```





